
# NLSpEL

This code is adapted from the following paper:

Shavarani, H. S., & Sarkar, A. (2023). SpEL: Structured prediction for entity linking. arXiv preprint arXiv:2310.14684.

The original source code is avaible [here](https://github.com/shavarani/SpEL).


## Installation

First, install dependencies, preferably in a venv:

```bash
 python -m pip install -r requirements.txt
```

Some packages might require older Python installations. To ensure compatability, you can use Python 3.10 and enter the following lines in the terminal:

```bash
sudo apt-get update
sudo apt-get install python3.10-distutils
python3.10 -m pip install -r requirements.txt
```

Currently, the checkpoints can be downloaded manually from Google Drive:

[nlspel-step-1-ddp.pt](https://drive.google.com/file/d/1vwEZYFbWio_caMNXJ3fiKgTxIpMFkA8m/view?usp=sharing)

[nlspel-step-2.pt](https://drive.google.com/file/d/1r6GmE2UbZQFDKohbVvJ3sMorv8KCAWgW/view?usp=sharing)

After downloading, make sure to add these files to the .checkpoints folder (or create this folder manually in NLSpEL/SpEL/)

Additionally, in order to continue fine-tuning the model, the following files should be downloaded and placed in SpEL/resources/data:

[MULTINERD-dataset.tsv](https://drive.google.com/file/d/1QTcKSq83PeH4Hjewoc6AmtSU8cgZ8_64/view?usp=sharing)
[multinerd_candidates.zip](https://drive.google.com/file/d/1EZa5dAQYVjTUmLRZfGD1LX3JK0bzO5KD/view?usp=sharing)

Afterwards, model can be used for inference like this:

```python
from transformers import AutoTokenizer
from spel.model import SpELAnnotator, dl_sa
from spel.configuration import device
from spel.utils import get_subword_to_word_mapping
from spel.span_annotation import WordAnnotation, PhraseAnnotation

finetuned_after_step = 2
sentence = "De antilopegrondeekhoorn (Ammospermophilus leucurus) is een zoogdier uit de familie van de eekhoorns (Sciuridae)."
tokenizer = AutoTokenizer.from_pretrained("pdelobelle/robbert-v2-dutch-ner")

# Load the model:
BERT_MODEL_NAME = "nlspel-step-2.pt"

spel = SpELAnnotator()
spel.init_model_from_scratch(BERT_MODEL_NAME=BERT_MODEL_NAME, device=device, finetuned_after_step=finetuned_after_step)

# Run NLSpEL:
inputs = tokenizer(sentence, return_tensors="pt")
token_offsets = list(zip(inputs.encodings[0].tokens,inputs.encodings[0].offsets))
subword_annotations = spel.annotate_subword_ids(inputs.input_ids, k_for_top_k_to_keep=10, token_offsets=token_offsets)

# Create word level annotations:
tokens_offsets = token_offsets[1:-1]
subword_annotations = subword_annotations[1:]
for sa in subword_annotations:
    sa.idx2tag = dl_sa.mentions_itos
word_annotations = [WordAnnotation(subword_annotations[m[0]:m[1]], tokens_offsets[m[0]:m[1]])
                    for m in get_subword_to_word_mapping(inputs.tokens(), sentence)]

# Create phrase level annotations:
phrase_annotations = []
for w in word_annotations:
    if not w.annotations:
        continue
    if phrase_annotations and phrase_annotations[-1].resolved_annotation == w.resolved_annotation:
        phrase_annotations[-1].add(w)
    else:
        phrase_annotations.append(PhraseAnnotation(w))

# Print out the created annotations:
for phrase_annotation in phrase_annotations:
   print(dl_sa.mentions_itos[phrase_annotation.resolved_annotation])
```

To see run the evaluation, call:
```bash
python3.10 evaluate_local.py
```

Inside this file, edit the BERT_MODEL_NAME variable to either nlspel-step-1-ddp.pt or nlspel-step-2.pt.

___


## Additional code

The code used to generate the training data is found in /nl_bert_entity/. It can be run by entering the following prompts:

```bash
python preprocess_all.py
```

Afterwards, the created pickle are stored in the data/versions folder and can be converted to json format:

```bash
python create_wiki_json.py
python create_multinerd_json.py
```

The pre-processing scripts are adapted from this paper:

Broscheit, S. (2020). Investigating entity knowledge in BERT with simple neural end-to-end entity linking. arXiv preprint arXiv:2003.05473.

The source code for this paper is avaible [here](https://github.com/samuelbroscheit/entity_knowledge_in_bert/tree/master/bert_entity/preprocessing).

___

The candidates can be generated by running the following in /candidate_generation/:

```bash
python create_entities_candidates.py
```

This script requires that the nlwiki/ folder is populated with the nlwiki-pages-articles files downloaded from the Wikipedia dump. These files can be downloaded directly from [dumps.wikimedia.org](https://dumps.wikimedia.org/nlwiki/), or can be copied from nl_bert_entity/data/downloads after running the pre-processing scripts. Additionally, the script needs the following files to be added to the same directory as the program:

[pagelinks-freq.txt](https://drive.google.com/file/d/1nA3wSz3Fg76Qw4u6EBuG0wxGfpljM1Ic/view?usp=sharing)
[pagelinks-counts.txt](https://drive.google.com/file/d/14xS2GBxoexWNb3X3mpS8RVfrzeY5rd12/view?usp=sharing)